{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from defusedxml import ElementTree as ET\n",
    "\n",
    "# # Fetching content\n",
    "# # url = input(\"paste url: \")\n",
    "# r = requests.get(\"https://hnrss.org/frontpage\")\n",
    "# # r.status_code == requests.codes.ok\n",
    "# content = r.text\n",
    "\n",
    "# # # Parsing data\n",
    "# root = ET.fromstring(content)\n",
    "# for title in root.iter('title'):\n",
    "#     print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from defusedxml import ElementTree as ET\n",
    "\n",
    "# Fetch the XML content\n",
    "def fetch_feed(url):\n",
    "    try:\n",
    "        # headers = {'User-Agent': 'Mozilla/5.0'}  If you need headers...\n",
    "        response = requests.get(url)  # Remember to add them to the requests\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch feed: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Extract namespaces for dynamic namespaces handling\n",
    "def extract_namespaces(xml_text):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    # namespaces dict to use for extraction\n",
    "    ns = {}\n",
    "   # Capture all namespaces from root\n",
    "    for key,value in root.attrib.items():\n",
    "        if key.startswith('xmlns'):\n",
    "            # Extract namespace prefix after colon\n",
    "            prefix = key.split(':')[-1] if ':' in key else 'atom'\n",
    "            ns[prefix] = value\n",
    "    return ns\n",
    "\n",
    "# Detect type of feed for correct parsing\n",
    "def detect_feed_type(root):\n",
    "    if root.tag.endswith('feed'):  # Atom feed\n",
    "        return 'atom'\n",
    "    elif root.tag == 'rss':  # RSS 2.0\n",
    "        return 'rss'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Title extraction: \n",
    "# (practice purpose, since we would need more than the title \n",
    "# for the real application).\n",
    "\n",
    "def get_feed_title(xml_text):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    ns = extract_namespaces(xml_text)\n",
    "\n",
    "    # Try both Atom and RSS formats\n",
    "    title = root.find('.//title', namespaces=ns) or \\\n",
    "            root.find('.//channel/title', namespaces=ns)\n",
    "\n",
    "    return title.text if title is not None else \"No title found\"\n",
    "\n",
    "# Parse the root element\n",
    "def parse_items(xml_text):\n",
    "    root = ET.fromstring(xml_text)\n",
    "    ns = extract_namespaces(xml_text)\n",
    "    \n",
    "    # Handle default namespaces\n",
    "    if not ns:\n",
    "        ns[''] = 'http://www.w3.org/2005/Atom'\n",
    "\n",
    "    items = []\n",
    "    # Atom format\n",
    "    for entry in root.findall('.//entry', namespaces=ns):\n",
    "        title = entry.find('.//title', namespaces=ns)\n",
    "        content = entry.find('.//content')\n",
    "        link = entry.find('.//atom:link[@href]', namespaces=ns)\n",
    "        if title is not None and content is not None and link is not None:\n",
    "            items.append({\n",
    "                'title': title.text,\n",
    "                'content': content.text,\n",
    "                'link': link.attrib.get('href', link.text)\n",
    "            })\n",
    "    # RSS Format\n",
    "    if not items:\n",
    "        for item in root.findall('.//item'):\n",
    "            title = item.find('title')\n",
    "            description = item.find('description')\n",
    "            link = item.find('link')\n",
    "            if title is not None and description is not None and link is not None:\n",
    "                items.append({\n",
    "                    'title': title.text,\n",
    "                    'description': description.text,\n",
    "                    'link': link.text\n",
    "                }) \n",
    "            \n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feeds = [\n",
    "#     (\"BBC (RSS 2.0)\", \"http://feeds.bbci.co.uk/news/rss.xml\"),\n",
    "#     (\"Reddit (Atom)\", \"https://www.reddit.com/r/python/.rss\"),\n",
    "#     (\"NASA (Atom)\", \"https://www.nasa.gov/feeds/iotd-feed/\")\n",
    "# ]\n",
    "\n",
    "# for name, url in test_feeds:\n",
    "#     parser = RSSParser(url)\n",
    "#     if parser.fetch():\n",
    "#         data = parser.parse()\n",
    "#         print(f\"\\n{name}:\")\n",
    "#         print(f\"Title: {data['title']}\")\n",
    "#         print(f\"First item: {data['items'][0]['title'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed: NASA\n",
      "Title:, How to Fly NASAâ€™s Orion Spacecraft, Link: https://www.nasa.gov/missions/artemis/how-to-fly-nasas-orion-spacecraft/\n"
     ]
    }
   ],
   "source": [
    "def debug():\n",
    "    url = \"https://www.nasa.gov/rss/dyn/breaking_news.rss\"\n",
    "    xml_text = fetch_feed(url)\n",
    "\n",
    "    # Check title\n",
    "    print(\"Feed:\", get_feed_title(xml_text))  # Should return Python\n",
    "\n",
    "    # Check items\n",
    "    items = parse_items(xml_text)\n",
    "    print(f\"Title:, {items[0]['title']}, Link: {items[0]['link']}\" )\n",
    "\n",
    "debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?><feed xmlns=\"http://www.w3.org/2005/Atom\" xmlns:media=\"http://\n"
     ]
    }
   ],
   "source": [
    "reddit_data = str(fetch_feed(\"https://www.reddit.com/r/python/.rss\"))\n",
    "print(reddit_data[:100])  # First 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GitHub Blog\n",
      "{'title': 'Considerations for making a tree view component accessible', 'desciption': '<p>A deep dive on the work that went into making the component that powers repository and pull request file trees.</p>\\n<p>The post <a href=\"https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/\">Considerations for making a tree view component accessible</a> appeared first on <a href=\"https://github.blog\">The GitHub Blog</a>.</p>\\n', 'link': 'https://github.blog/engineering/user-experience/considerations-for-making-a-tree-view-component-accessible/'}\n"
     ]
    }
   ],
   "source": [
    "# Test with Atom feed\n",
    "reddit_data = fetch_feed(\"https://github.blog/feed/\")\n",
    "print(get_feed_title(reddit_data))  # Should return \"Python\"\n",
    "print(parse_items(reddit_data)[0])  # First post's title/link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC News\n",
      "{'title': \"Woman arrested for second time over girls' school crash deaths\", 'desciption': 'Nuria Sajjad and Selena Lau were at a school tea party when a Land Rover crashed into the playground.', 'link': 'https://www.bbc.com/news/articles/cm27dg7e7ddo'}\n"
     ]
    }
   ],
   "source": [
    "# Test with RSS 2.0\n",
    "bbc_data = fetch_feed(\"http://feeds.bbci.co.uk/news/rss.xml\")\n",
    "print(get_feed_title(bbc_data))  # \"BBC News - Home\"\n",
    "print(parse_items(bbc_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_data = fetch_feed(\"https://hnrss.org/frontpage\")\n",
    "# if feed_data:\n",
    "#     print(extract_namespace(feed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_data = fetch_feed(\"https://www.reddit.com/r/python/.rss\")\n",
    "# if feed_data:\n",
    "#     print(feed_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# feed_data = fetch_feed(\"https://www.reddit.com/r/python/.rss\")\n",
    "# if feed_data:\n",
    "#     print(get_feed_title(feed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
